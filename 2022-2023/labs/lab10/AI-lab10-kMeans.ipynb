{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Probleme de clasificare nesupervizata (clustering) <img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c8/Cluster-2.svg\" width=\"150\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obiective\r\n",
    "* rezolvarea unei probleme de clustering (metoda bazata pe k-means)\r\n",
    "* extragerea de caracteristici din informatiile textuale\r\n",
    "\r\n",
    "## Cuvinte cheie:\r\n",
    "* date de antrenare si date de testare \r\n",
    "* atribute/catacteristici ale datelor\r\n",
    "* etichete ale datelor\r\n",
    "* normalizare date \r\n",
    "* model de clusterizare\r\n",
    "* acuratetea clusterizarii"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aspecte teoretice"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clasificarea nesupervizata, numita si clusterizare, are drept scop invatarea unei modalitati de grupare a datelor pentru care nu au fost oferite in prealabil detalii despre apartententa lor la anumite clase. In loc sa raspunda la un feedback (construit pe baza etichetelor asociate exemplelor de antrenament), clasificarea nesuepervizata identifica asemanari intre date. \r\n",
    "\r\n",
    "Se doreste impartirea unor exemple neetichetate în submultimi disjuncte (clusteri) astfel incat:\r\n",
    "* exemplele din acelasi cluster sunt foarte similare\r\n",
    "* exemplele din clusteri diferiti sunt foarte diferite\r\n",
    "\r\n",
    "\r\n",
    "### Exemple de probleme de clusterizare:\r\n",
    "* segmentarea clientilor de pe o anumita piata  \r\n",
    "* analize in retele sociale (identificarea unor grupuri cu anumite particularitati)\r\n",
    "* compresia imaginilor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<details>\r\n",
    "  <summary>Remember the theory behind clustering </summary>\r\n",
    " \r\n",
    "\r\n",
    "### Formalizare problema de clusterizare:\r\n",
    "\r\n",
    "Clusterizarea presupune identificarea unui model de asociere exemplu-eticheta.\r\n",
    "Se dau:\r\n",
    "* un set de date de train si un set de date de test\r\n",
    "* fiecare data (exemplu) se caracterizeaza prin $m$ $atribute$ ($x = (x_1, x_2, ..., x_m)$) (atributele pot fi valori numerice sau valori nenumerice)\r\n",
    "\r\n",
    "Se cere sa se determine o functie (necunoscuta) care realizeaza gruparea datelor de antrenament in mai multe clase\r\n",
    "* nr de clase poate fi pre-definit (k) sau necunoscut\r\n",
    "* datele dintr-o clasa sunt asemanatoare\r\n",
    "* clasa asociata unei date (noi) de test folosind gruparea invatata pe datele de antrenament\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "### Algoritmi\r\n",
    "* k-means <img src=\"http://shabal.in/visuals/kmeans/random.gif\" width=\"150\">\r\n",
    "\r\n",
    "\r\n",
    "Logica k-means:\r\n",
    "\r\n",
    "1. Se initializeaza $k$ centroizi $\\mu_1$, $\\mu_2$, ..., $\\mu_{k}$. Un centroid $\\mu_j$ este un vector cu $m$ valori ($m$ - nr de atribute). Initializarea se poate realiza:\r\n",
    "* pur random\r\n",
    "* alegand, random, $k$ exemple (din setul de date de antrenament)\r\n",
    "\r\n",
    "2. Se repeta pana la convergenta: \r\n",
    "\r\n",
    "- Fiecare exemplu $x^i$ din setul de antrenament se asociaza celui mai apropiat centroid (i se asociaza eticheta $c^i$)\r\n",
    "$$c^i = \\arg\\min_{j=1,2,  \\ldots, k}{dist(x^i, \\mu_j)},$$\r\n",
    "\r\n",
    "unde $dist(a,b)$ este o functie care masoara distanta intre 2 exemple $a$ si $b$ (intre atributele lor).\r\n",
    "\r\n",
    "- Se recalculeaza centroizii prin mutarea lor in media exemplelor asociate fiecaruia\r\n",
    "\r\n",
    "$$ \\mu_j = \\frac{\\sum_{i=1,2, \\ldots, N}{1_{c^i=j} x^i}}{\\sum_{i=1,2, \\ldots, N}{1_{c^i=j}}}$$ \r\n",
    "\r\n",
    "\r\n",
    "</details>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exemple"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Demo1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Problema: Ce fel de mesaje primesti in Inbox? \r\n",
    "Se doreste clusterizarea unor mesaje in doua categorii (spam si ham). Pentru fiecare mesaj se cunoaste textul aferent lui. Să se rezolve problema, implementându-se rutine pentru clusterizare cu k-means a mesajelor.\r\n",
    "\r\n",
    "Proces:\r\n",
    "* Se pleaca de la un set de date format din textul mesajelor precum cel din fisierul spam csv [link](data/spam.csv).\r\n",
    "* Se imparte setul de date in date de antrenament si in date de test\r\n",
    "* Se extrag anumite caracteristici din textul mesajelor folosind diferite reprezentari precum:\r\n",
    "    - Bag of Words\r\n",
    "    - TF-IDF\r\n",
    "    - Word2Vec\r\n",
    "* invatare model - se aplica algoritmul k-means pe setul de antrenament si se identifica cei doi centroizi (corespunzatori clusterilor spam si ham, respectiv)\r\n",
    "* stabilire rezultate (pt datele de test) - fiecare mesaj din setul de test se asociaza acelui cluster pentru care distanta dintre caracteristicile mesajului si centroid este minima\r\n",
    "* calcul metrici de performanta"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import csv\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pasul 1 - incarcare date"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# load some data\r\n",
    "crtDir =  os.getcwd()\r\n",
    "fileName = os.path.join(crtDir, 'data', 'spam.csv')\r\n",
    "\r\n",
    "data = []\r\n",
    "with open(fileName) as csv_file:\r\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\r\n",
    "    line_count = 0\r\n",
    "    for row in csv_reader:\r\n",
    "        if line_count == 0:\r\n",
    "            dataNames = row\r\n",
    "        else:\r\n",
    "            data.append(row)\r\n",
    "        line_count += 1\r\n",
    "\r\n",
    "inputs = [data[i][0] for i in range(len(data))]\r\n",
    "outputs = [data[i][1] for i in range(len(data))]\r\n",
    "labelNames = list(set(outputs))\r\n",
    "\r\n",
    "print(inputs[:2])\r\n",
    "print(labelNames[:2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...']\n",
      "['ham', 'spam']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pasul 2 - impartire date (antrenament si test)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# prepare data for training and testing\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "np.random.seed(5)\r\n",
    "# noSamples = inputs.shape[0]\r\n",
    "noSamples = len(inputs)\r\n",
    "indexes = [i for i in range(noSamples)]\r\n",
    "trainSample = np.random.choice(indexes, int(0.8 * noSamples), replace = False)\r\n",
    "testSample = [i for i in indexes  if not i in trainSample]\r\n",
    "\r\n",
    "trainInputs = [inputs[i] for i in trainSample]\r\n",
    "trainOutputs = [outputs[i] for i in trainSample]\r\n",
    "testInputs = [inputs[i] for i in testSample]\r\n",
    "testOutputs = [outputs[i] for i in testSample]\r\n",
    "\r\n",
    "print(trainInputs[:3])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Probably, want to pick up more?', \"No go. No openings for that room 'til after thanksgiving without an upcharge.\", \"Fuck babe ... I miss you already, you know ? Can't you let me send you some money towards your net ? I need you ... I want you ... I crave you ...\"]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pasul 3 - extragere caracteristici"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# extract some features from the raw text\r\n",
    "\r\n",
    "# # representation 1: Bag of Words\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "vectorizer = CountVectorizer()\r\n",
    "\r\n",
    "trainFeatures = vectorizer.fit_transform(trainInputs)\r\n",
    "testFeatures = vectorizer.transform(testInputs)\r\n",
    "\r\n",
    "# vocabbulary from the train data \r\n",
    "print('vocab: ', vectorizer.get_feature_names()[:10])\r\n",
    "# extracted features\r\n",
    "print('features: ', trainFeatures.toarray()[:3][:10])\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocab:  ['00', '000', '000pes', '008704050406', '0089', '0121', '01223585236', '01223585334', '0125698789', '02']\n",
      "features:  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# representation 2: tf-idf features - word granularity\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "vectorizer = TfidfVectorizer(max_features=50)\r\n",
    "\r\n",
    "trainFeatures = vectorizer.fit_transform(trainInputs)\r\n",
    "testFeatures = vectorizer.transform(testInputs)\r\n",
    "\r\n",
    "# vocabbulary from the train data \r\n",
    "print('vocab: ', vectorizer.get_feature_names()[:10])\r\n",
    "# extracted features\r\n",
    "print('features: ', trainFeatures.toarray()[:3])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocab:  ['all', 'and', 'are', 'at', 'be', 'but', 'call', 'can', 'do', 'for']\n",
      "features:  [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.48953734 0.87198234\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.32963209 0.         0.\n",
      "  0.         0.41437418 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.77258758 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.35035005 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.20529303 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.23693671\n",
      "  0.         0.         0.17904984 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.91380454 0.18605964]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# representation 3: embedded features extracted by a pre-train model (in fact, word2vec pretrained model)\r\n",
    "\r\n",
    "import gensim \r\n",
    "\r\n",
    "# Load Google's pre-trained Word2Vec \r\n",
    "crtDir =  os.getcwd()\r\n",
    "modelPath = os.path.join(crtDir, 'models', 'GoogleNews-vectors-negative300.bin')\r\n",
    "\r\n",
    "word2vecModel300 = gensim.models.KeyedVectors.load_word2vec_format(modelPath, binary=True) \r\n",
    "print(word2vecModel300.most_similar('support'))\r\n",
    "print(\"vec for house: \", word2vecModel300[\"house\"])\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('supporting', 0.6251285076141357), ('suport', 0.6071150302886963), ('suppport', 0.6053199768066406), ('Support', 0.6044273376464844), ('supported', 0.6009396910667419), ('backing', 0.6007589101791382), ('supports', 0.5269277095794678), ('assistance', 0.5207138061523438), ('sup_port', 0.5192490220069885), ('supportive', 0.5110025405883789)]\n",
      "vec for house:  [ 1.57226562e-01 -7.08007812e-02  5.39550781e-02 -1.89208984e-02\n",
      "  9.17968750e-02  2.55126953e-02  7.37304688e-02 -5.68847656e-02\n",
      "  1.79687500e-01  9.27734375e-02  9.03320312e-02 -4.12109375e-01\n",
      " -8.30078125e-02 -1.45507812e-01 -2.37304688e-01 -3.68652344e-02\n",
      "  8.74023438e-02 -2.77099609e-02  1.13677979e-03  8.30078125e-02\n",
      "  3.57421875e-01 -2.61718750e-01  7.47070312e-02 -8.10546875e-02\n",
      " -2.35595703e-02 -1.61132812e-01 -4.78515625e-02  1.85546875e-01\n",
      " -3.97949219e-02 -1.58203125e-01 -4.37011719e-02 -1.11328125e-01\n",
      " -1.05957031e-01  9.86328125e-02 -8.34960938e-02 -1.27929688e-01\n",
      " -1.39648438e-01 -1.86523438e-01 -5.71289062e-02 -1.17675781e-01\n",
      " -1.32812500e-01  1.55639648e-02  1.34765625e-01  8.39843750e-02\n",
      " -9.03320312e-02 -4.12597656e-02 -2.51953125e-01 -2.27539062e-01\n",
      " -6.64062500e-02 -7.66601562e-02  5.15136719e-02  5.90820312e-02\n",
      "  3.49609375e-01 -1.13769531e-01 -2.57568359e-02 -1.98242188e-01\n",
      "  4.44335938e-02  1.09863281e-01  1.04003906e-01 -1.75781250e-01\n",
      "  1.22558594e-01  7.81250000e-02  6.20117188e-02  6.49414062e-02\n",
      " -1.73828125e-01 -1.11694336e-02  1.88476562e-01  3.34472656e-02\n",
      " -4.29687500e-02 -4.71191406e-02  2.91015625e-01  4.19921875e-02\n",
      "  1.59179688e-01  1.22558594e-01 -2.55859375e-01  2.44140625e-01\n",
      "  1.54296875e-01 -3.46679688e-02  1.24023438e-01 -1.32812500e-01\n",
      "  8.44726562e-02  3.71093750e-02 -1.05468750e-01  9.81445312e-02\n",
      " -8.23974609e-03  5.34667969e-02 -1.96838379e-03  9.03320312e-02\n",
      "  1.30859375e-01 -1.57470703e-02 -2.40478516e-02 -3.29589844e-02\n",
      " -5.63964844e-02 -3.12500000e-01 -1.19140625e-01  4.41894531e-02\n",
      " -1.82617188e-01 -2.20703125e-01  8.39843750e-02 -2.15820312e-01\n",
      " -1.60156250e-01 -2.01171875e-01  1.63085938e-01 -4.57763672e-05\n",
      "  4.24804688e-02 -1.37695312e-01 -2.62451172e-02  1.53320312e-01\n",
      " -1.07421875e-01 -1.34765625e-01 -3.73840332e-03 -1.51977539e-02\n",
      " -7.27539062e-02  3.22265625e-02  1.89453125e-01 -8.00781250e-02\n",
      "  1.45507812e-01 -9.66796875e-02 -9.27734375e-02  8.91113281e-03\n",
      " -4.27246094e-02 -9.76562500e-02  3.29589844e-02 -7.95898438e-02\n",
      " -6.25000000e-02  3.39355469e-02  1.05590820e-02 -1.28906250e-01\n",
      "  1.09863281e-01  1.89453125e-01  1.52343750e-01 -1.47460938e-01\n",
      " -3.86047363e-03  1.75781250e-01 -4.58984375e-02 -1.02539062e-01\n",
      "  6.34765625e-02 -9.86328125e-02  1.87500000e-01  3.97949219e-02\n",
      " -2.65625000e-01 -1.24023438e-01 -1.35742188e-01  7.93457031e-03\n",
      "  6.59179688e-02  8.11767578e-03 -3.24707031e-02 -1.03759766e-02\n",
      " -1.90429688e-02 -8.20312500e-02  2.06054688e-01  1.40625000e-01\n",
      "  1.93359375e-01  2.91015625e-01 -9.17968750e-02 -1.40625000e-01\n",
      " -1.75781250e-01 -1.36718750e-01  2.51464844e-02 -5.83496094e-02\n",
      " -1.84570312e-01  3.10546875e-01  7.17773438e-02 -1.01074219e-01\n",
      "  1.08886719e-01 -2.23388672e-02  1.50390625e-01 -7.03125000e-02\n",
      "  1.24023438e-01  2.21679688e-01 -1.97265625e-01 -6.05468750e-02\n",
      " -4.30297852e-03 -1.69921875e-01 -1.45507812e-01 -2.17773438e-01\n",
      "  2.47070312e-01  6.64062500e-02 -8.05664062e-02  3.57421875e-01\n",
      " -8.20312500e-02 -7.87353516e-03  1.08886719e-01 -5.32226562e-02\n",
      "  3.00781250e-01 -2.37304688e-01  1.61132812e-01  1.59179688e-01\n",
      "  1.69921875e-01 -9.52148438e-02  5.20019531e-02 -6.22558594e-02\n",
      " -8.85009766e-03  4.68750000e-02 -2.88085938e-02  1.25000000e-01\n",
      "  3.49121094e-02  4.61425781e-02  1.66015625e-02 -9.57031250e-02\n",
      " -1.48437500e-01 -1.64794922e-02 -2.22656250e-01 -2.51953125e-01\n",
      " -3.58886719e-02 -2.52685547e-02  8.39233398e-05  6.98242188e-02\n",
      "  2.53906250e-01 -3.29589844e-02  6.59179688e-02  6.28662109e-03\n",
      " -7.86132812e-02 -3.01513672e-02 -9.47265625e-02  1.25000000e-01\n",
      " -1.62109375e-01  2.53906250e-01 -3.30078125e-01  6.44531250e-02\n",
      " -9.09423828e-03  7.12890625e-02  3.99780273e-03 -4.41894531e-02\n",
      " -1.42822266e-02 -9.52148438e-03  1.17675781e-01  3.49609375e-01\n",
      " -2.90527344e-02  1.86767578e-02  3.46679688e-02  1.89208984e-02\n",
      " -1.26953125e-01  2.68554688e-02 -1.06933594e-01  1.20117188e-01\n",
      " -2.69775391e-02 -5.07812500e-02 -1.76757812e-01  3.95507812e-02\n",
      "  1.35742188e-01 -9.61914062e-02 -1.98242188e-01 -1.86767578e-02\n",
      " -2.47802734e-02 -5.32226562e-02  1.54296875e-01  5.95703125e-02\n",
      "  6.39648438e-02 -6.17675781e-02  3.36914062e-02  1.75781250e-01\n",
      "  6.59179688e-02  2.22656250e-01 -1.28906250e-01  4.61425781e-02\n",
      " -2.57812500e-01  6.78710938e-02  6.29882812e-02 -1.15722656e-01\n",
      " -2.13867188e-01 -2.53906250e-01  2.73437500e-02 -4.68750000e-02\n",
      "  1.38671875e-01  2.59765625e-01 -2.07031250e-01 -9.64355469e-03\n",
      " -5.22460938e-02 -7.20214844e-03  8.49609375e-02 -2.49023438e-02\n",
      "  1.94335938e-01 -7.37304688e-02  1.22070312e-01 -3.49121094e-02\n",
      "  1.41601562e-01 -1.38671875e-01  7.61718750e-02 -1.93359375e-01\n",
      "  1.64062500e-01 -2.78320312e-02 -1.45263672e-02  1.44531250e-01\n",
      "  1.75781250e-01 -1.70898438e-02  1.26953125e-01  3.39355469e-02\n",
      " -2.80761719e-02  1.82617188e-01 -2.94921875e-01  3.78417969e-02\n",
      " -1.63085938e-01  1.73828125e-01 -1.01074219e-01 -1.49414062e-01\n",
      " -4.17480469e-02  9.82666016e-03 -4.94384766e-03 -3.29589844e-02]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def featureComputation(model, data):\r\n",
    "    features = []\r\n",
    "    phrases = [ phrase.split() for phrase in data]\r\n",
    "    for phrase in phrases:\r\n",
    "        # compute the embeddings of all the words from a phrase (words of more than 2 characters) known by the model\r\n",
    "        vectors = [model[word] for word in phrase if (len(word) > 2) and (word in model.vocab.keys())]\r\n",
    "        if len(vectors) == 0:\r\n",
    "            result = [0.0] * model.vector_size\r\n",
    "        else:\r\n",
    "            result = np.sum(vectors, axis=0) / len(vectors)\r\n",
    "        features.append(result)\r\n",
    "    return features\r\n",
    "\r\n",
    "trainFeatures = featureComputation(word2vecModel300, trainInputs)\r\n",
    "testFeatures = featureComputation(word2vecModel300, testInputs)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pasul 4 - antrenare model de invatare nesupervizata (clustering)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# unsupervised classification ( = clustering) of data\r\n",
    "\r\n",
    "from sklearn.cluster import KMeans\r\n",
    "\r\n",
    "unsupervisedClassifier = KMeans(n_clusters=2, random_state=0)\r\n",
    "unsupervisedClassifier.fit(trainFeatures)\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pasul 5 - testare model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "computedTestIndexes = unsupervisedClassifier.predict(testFeatures)\r\n",
    "computedTestOutputs = [labelNames[value] for value in computedTestIndexes]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pasul 6 - calcul metrici de performanta"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "# just supposing that we have the true labels\r\n",
    "print(\"acc: \", accuracy_score(testOutputs, computedTestOutputs))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "acc:  0.8582959641255605\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tema"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Problema: Clasificarea textelor pe baza sentimentelor\r\n",
    " \r\n",
    " **Retea sociala: ce fel de mesaje ai postat?**\r\n",
    "Mai tii minte ca tocmai ti-ai inceput munca ca si software developer la Facebook si ca faci parte din echipa care se ocupa cu partea de continut a platformei? \r\n",
    "\r\n",
    "Utilizatorii sunt foarte incantati de noul algoritm de detectie a filtrelor in poze, asadar poti sa te ocupi de o noua functionalitate care ar face platforma mai atractiva. Utilizatorii posteaza o gama larga de mesaje, iar in feed-urile lor apar de multe ori prea multe mesaje negative si prea putine pozitive. Facebook incearca o noua functionalitate prin care sa detecteze sentimentele dintr-un mesaj si sa filtreze feed-urile utilizatorilor.\r\n",
    "\r\n",
    "Task-ul tau este sa implementezi un algoritm care poate recunoaste sentimentele dintr-un text (pozitiv, negativ, ura, rasism, etc.)\r\n",
    "\r\n",
    "Team leaderul echipei de ML iti propune urmatorul plan de lucru \r\n",
    "- devoltarea, antrenarea si testarea unui algoritm de tip k-means folosind data de tip numeric (de ex datele cu irisi) \r\n",
    "- devoltarea, antrenarea si testarea unui algoritm de tip k-means folosind data de tip text\r\n",
    "    - Considerarea unei baze cu texte etichetate cu emotii (de ex. textele din data/review_mixed.csv sau https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets)\r\n",
    "    - Extragerea de caracteristici din texte folosind diferite reprezentari precum:\r\n",
    "        - Bag of Words\r\n",
    "        - TF-IDF\r\n",
    "        - Word2Vec\r\n",
    "        - N-grams, etc.\r\n",
    "    - pe baza caracteristicilor extrase, clasificarea textelor si etichetarea lor cu emotii folosind\r\n",
    "        - un algoritm de invatare supervizat (folosind etichetele pt emotiile asociate fiecarui text)\r\n",
    "        - un algoritm de invatare nesupervizat bazat pe k-means (fara a folosi etichetele pt emotiile asociate fiecarui text)\r\n",
    "        - un algoritm hibrid care combina tehncile de invare cu reguli ajutatoare, de ex prin folosirea unor reguli care verifica/numara aparitiile unor cuvinte - polarized words - (e.g. negative words such as bad, worst, ugly, etc and positive words such as good, best, beautiful, etc.)\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.2 64-bit"
  },
  "interpreter": {
   "hash": "b15e34aad66e9e3b527f162cfcae96454e420cf4f38bc5947ead173934d3535a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}